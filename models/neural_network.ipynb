{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "999dc25e-32d9-4545-acf7-397a5b93e456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fae539-6f44-4df8-96f4-5a8ecba231af",
   "metadata": {},
   "source": [
    "# Simple Neural Network (Rede Neural Simples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "85ef0685-2722-401a-9c3b-a6f0424848fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense(a_in, W, b, activation='linear'):\n",
    "    \"\"\"\n",
    "    Computes dense layer\n",
    "    Args:\n",
    "      a_in (ndarray (n, )) : Data, 1 example \n",
    "      W    (ndarray (n,j)) : Weight matrix, n features per unit, j units\n",
    "      b    (ndarray (j, )) : bias vector, j units  \n",
    "    Returns\n",
    "      a_out (ndarray (j,))  : j units\n",
    "    \"\"\"\n",
    "\n",
    "    units = W.shape[1]\n",
    "    a_out = np.zeros(units)\n",
    "\n",
    "    for j in range(units):\n",
    "        w = W[:, j]\n",
    "        z = np.dot(w, a_in) + b[j]\n",
    "\n",
    "        if activation == 'sigmoid':\n",
    "            a_out[j] += 1 / (1 + np.exp(-z))\n",
    "        elif activation == 'relu':\n",
    "            a_out[j] += np.max(z)\n",
    "        elif activation == 'softmax':\n",
    "            a_out[j] += np.exp(Z) / np.sum(np.exp(Z))\n",
    "        else:\n",
    "            a_out[j] = z\n",
    "    return a_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "708d2f56-1dcc-4918-af10-25d893259e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential(X, W1, b1, W2, b2, dense):\n",
    "    a1 = dense(X, W1, b1)\n",
    "    a2 = dense(a1, W2, b2)\n",
    "    return a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d3b9286e-d286-4e15-a956-4cd5fd7f414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W1, b1, W2, b2, sequential, dense):\n",
    "    m, _ = X.shape\n",
    "    yhat = np.zeros((m, 1))\n",
    "\n",
    "    for i in range(m):\n",
    "        yhat[i, 0] = sequential(X[i], W1, b1, W2, b2, dense)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5887717c-f270-47b5-97c5-75ff7c8cec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.array( [[-8.93,  0.29, 12.9 ], [-0.1,  -7.32, 10.81]] )\n",
    "b1 = np.array( [-9.82, -9.28,  0.96] )\n",
    "W2 = np.array( [[-31.18], [-27.59], [-32.56]] )\n",
    "b2 = np.array( [15.41] )\n",
    "X = np.array([\n",
    "    [200,13.9],\n",
    "    [200,17]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "03299d1c-a2bd-4c41-ab11-3d671cf2658c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decisions = [[-31413.03032]\n",
      " [-31868.4104 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\02715067\\AppData\\Local\\Temp\\ipykernel_22852\\904062579.py:6: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  yhat[i, 0] = sequential(X[i], W1, b1, W2, b2, dense)\n"
     ]
    }
   ],
   "source": [
    "yhat = predict(X, W1, b1, W2, b2, sequential, dense)\n",
    "#yhat = (predictions >= 0.5).astype(int)\n",
    "print(f'decisions = {yhat}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0f0f00-81e7-4782-bc3b-b74d3e8c5d96",
   "metadata": {},
   "source": [
    "# Vectorized Implementation (Implementação Vetorizada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b077f59c-15c7-4956-aede-9f9f75d0f779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_vectorized(A_in, W, B, activation='linear'):\n",
    "    Z = np.matmul(A_in, W) + B # Ou/Or, Z = A_in @ W + B\n",
    "    \n",
    "    if activation == 'sigmoid':\n",
    "        A_out = 1 / (1 + np.exp(-Z))\n",
    "    elif activation == 'relu':\n",
    "        A_out = np.max(Z)\n",
    "    elif activation == 'softmax':\n",
    "        A_out = np.exp(Z) / np.sum(np.exp(Z))\n",
    "    else:\n",
    "        return Z\n",
    "    return A_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c4d48d-6e02-4fc1-83a3-965df0a4fc4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
