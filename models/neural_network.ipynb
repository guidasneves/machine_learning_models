{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "106a8094-aae9-4913-8329-508b431733f3",
   "metadata": {},
   "source": [
    "# Neural Network (Rede Neural)\n",
    "**[EN-US]**\n",
    "\n",
    "Each layer ($l$) inserts a vector of numbers to the next layer, where the neurons of the next layer apply a linear function ($\\vec{z}$) and then a non-linear function, activation ($g(\\vec {z})$) to the vector, then this new vector of numbers is passed to the next layer, which will perform the same calculation, and the vector is passed from layer to layer until reaching the final calculation of the output layers, which is neural network prediction.\n",
    "\n",
    "We can use the notation $a^{[0]} = X$, that is, $X$, the $X$ matrix of the input layer, the initial layer of our neural network, is considered as $a^{[0] }$, the activation on layer 0.\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "Cada layer ($l$) insere um vetor de números à próxima layer, onde os neurônios da próxima layer aplicam uma função linear ($\\vec{z}$) e depois uma função não linear, a ativação ($g(\\vec{z})$) ao vetor, em seguida, esse novo vetor de números é passado para a próxima layer, que realizará o mesmo cálculo, e o vetor é passado de layer em layer até chegar ao cálculo final das outputs layers, que é a previsão da rede neural.\n",
    "\n",
    "Podemos usar a notação $a^{[0]} = X$, ou seja, $X$, a matriz $X$ da input layer, a layer inicial da nossa rede neural, é considerada como $a^{[0]}$, a ativação na layer 0.\n",
    "\n",
    "## Table of Contents\n",
    "* [Libraries](#Libraries)\n",
    "* [Simples Neural Network](#Simple-Neural-Network-(Rede-Neural-Simples))\n",
    "    * [Activations](#Activations-(Ativações))\n",
    "    * [Dense Layer](#Dense-Layer-(Câmada-Densa))\n",
    "    * [Sequential](#Sequential-(Sequencial))\n",
    "* [Vectorize Implementation](#Vectorized-Implementation-(Implementação-Vetorizada))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1783a9-ebc0-444b-ad3b-49889462dc93",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "999dc25e-32d9-4545-acf7-397a5b93e456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fae539-6f44-4df8-96f4-5a8ecba231af",
   "metadata": {},
   "source": [
    "## Simple Neural Network (Rede Neural Simples)\n",
    "### Activations (Ativações)\n",
    "**[EN-US]**\n",
    "\n",
    "`Hidden layer`: refers to the fact that in the training set the true values for these `hidden units` are not observed, that is, we do not see what they should be in the training set. We see what the inputs and outputs are, what the outputs should be, but the things in the hidden layers are not seen in the training set.\n",
    "\n",
    "`Activations`: refers to the values that different layers of the neural network are passing to subsequent layers. Activation functions:\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "`Hidden layer`: refere-se ao fato que no training set os valores verdadeiros para essas `hidden units` nã são observados, ou seja, não vemos o que eles deveriam ser no training set. Vemos quais são os inputs e os outputs, o que os outputs deveriam ser, mas as coisas nas hidden layers não são vistas no training set.\n",
    "\n",
    "`Activations`: refere-se aos valores que diferentes layers da rede neural estão passando para as layers subsequentes. Funções de ativação:\n",
    "$$z = \\mathbf{w} \\cdot \\mathbf{X} + b$$\n",
    "- linear/no activation (sem ativação):\n",
    "$$a = g(z) = Z$$\n",
    "- sigmoid:\n",
    "$$a = g(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "- tanh:\n",
    "$$a = g(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$$\n",
    "- ReLU (Rectified Linear Unit):\n",
    "$$a = g(z) = max(0,\\ z)$$\n",
    "- Leaky ReLU:\n",
    "$$a = g(z) = max(0.01 z,\\ z)$$\n",
    "- softmax:\n",
    "$$a_j = g(z_j) = \\frac{e^{z_j}}{\\sum\\limits_{k=0}^{N-1}e^{z_k}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25afd0d1-d8c4-424d-bdb5-9306ca2ac680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    ez = np.exp(z)\n",
    "    a = ez / np.sum(ez)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19109811-d492-4052-8364-63de71ded927",
   "metadata": {},
   "source": [
    "### Dense Layer (Câmada Densa)\n",
    "**[EN-US]**\n",
    "\n",
    "Because every feature in the input layer is interconnected with every neuron in the next layer, and so on.\n",
    "* We can call it either Dense, or Fully Connected (FC), or Flatten Layer.\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "Porque todas as features da input layer estão interconectadas com cada neurônio da próxima layer, e assim por diante.\n",
    "* Podemos chamá-la ou de Dense, ou de Fully Connected (FC), ou de Flatten layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85ef0685-2722-401a-9c3b-a6f0424848fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dense(a_in, W, b, activation='linear'):\n",
    "    units = W.shape[1]\n",
    "    a_out = np.zeros(units)\n",
    "\n",
    "    for j in range(units):\n",
    "        w = W[:, j]\n",
    "        Z = np.dot(w, a_in) + b[j]\n",
    "\n",
    "        if activation == 'sigmoid':\n",
    "            a_out[j] += 1 / (1 + np.exp(-Z))\n",
    "        elif activation == 'tanh':\n",
    "            a_out[j] += (np.exp(Z) - np.exp(-Z)) / (np.exp(Z) + np.exp(-Z))\n",
    "        elif activation == 'relu':\n",
    "            a_out[j] += np.maximum(0, Z)\n",
    "        elif activation == 'leaky_relu':\n",
    "            a_out[j] += np.maximum(0.01 * Z, Z)\n",
    "        elif activation == 'softmax':\n",
    "            a_out[j] += np.exp(Z) / np.sum(np.exp(Z))\n",
    "        else:\n",
    "            a_out[j] = z\n",
    "    return a_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a70c3dc-5e51-42a6-bf6b-182e91e3196d",
   "metadata": {},
   "source": [
    "### Sequential (Sequencial)\n",
    "**[EN-US]**\n",
    "\n",
    "It will join the layers sequentially.\n",
    "\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "Juntará as layers sequencialmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "708d2f56-1dcc-4918-af10-25d893259e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential(X, W1, b1, W2, b2, dense):\n",
    "    a1 = dense(X, W1, b1)\n",
    "    a2 = dense(a1, W2, b2)\n",
    "    return a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d3b9286e-d286-4e15-a956-4cd5fd7f414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W1, b1, W2, b2, sequential, dense):\n",
    "    m, _ = X.shape\n",
    "    yhat = np.zeros((m, 1))\n",
    "\n",
    "    for i in range(m):\n",
    "        yhat[i, 0] = sequential(X[i], W1, b1, W2, b2, dense)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5887717c-f270-47b5-97c5-75ff7c8cec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.array( [[-8.93,  0.29, 12.9 ], [-0.1,  -7.32, 10.81]] )\n",
    "b1 = np.array( [-9.82, -9.28,  0.96] )\n",
    "W2 = np.array( [[-31.18], [-27.59], [-32.56]] )\n",
    "b2 = np.array( [15.41] )\n",
    "X = np.array([\n",
    "    [200,13.9],\n",
    "    [200,17]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03299d1c-a2bd-4c41-ab11-3d671cf2658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = predict(X, W1, b1, W2, b2, sequential, dense)\n",
    "print(f'decisions = {yhat}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0f0f00-81e7-4782-bc3b-b74d3e8c5d96",
   "metadata": {},
   "source": [
    "## Vectorized Implementation (Implementação Vetorizada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b077f59c-15c7-4956-aede-9f9f75d0f779",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dense_vectorized(A_in, W, B, activation='linear'):\n",
    "    Z = np.matmul(A_in, W) + B # Ou/Or, Z = A_in @ W + B\n",
    "    \n",
    "    if activation == 'sigmoid':\n",
    "        A_out = 1 / (1 + np.exp(-Z))\n",
    "    elif activation == 'tanh':\n",
    "        A_out += (np.exp(Z) - np.exp(-Z)) / (np.exp(Z) + np.exp(-Z))\n",
    "    elif activation == 'relu':\n",
    "        A_out += np.maximum(0, Z)\n",
    "    elif activation == 'leaky_relu':\n",
    "        A_out += np.maximum(0.01 * Z, Z)\n",
    "    elif activation == 'softmax':\n",
    "        A_out = np.exp(Z) / np.sum(np.exp(Z))\n",
    "    else:\n",
    "        return Z\n",
    "    return A_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c4d48d-6e02-4fc1-83a3-965df0a4fc4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
