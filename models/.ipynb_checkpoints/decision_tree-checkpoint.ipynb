{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5d06763-e531-4158-93aa-977982b7f08f",
   "metadata": {},
   "source": [
    "# Decision Tree (Árvore de Decisão)\n",
    "**[EN-US]**\n",
    "\n",
    "The algorithm starts at the root node, where we select the best feature to make the split at this node based on `Information Gain`, where the values are divided between the left and right node, where we select other features to divide the data in these decision nodes, and so on, until the final node/leaf node where the algorithm makes the prediction.\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "O algoritmo começa no nó raiz (root node), onde selecionamos a melhor feature para fazer a divisão nesse nó com base no `Information Gain`, onde os valores são divididos entre o nó da esquerda e da direita, onde selecionamos outras features para dividir os dados nesses nós (decision nodes), e assim por diante, até o nó final (leaf node) onde o algoritmo faz a previsão. \n",
    "\n",
    "## Table of Contents\n",
    "* [Libraries](#Libraries)\n",
    "* [Decision Tree (Árvore de Decisão)](#Decision-Tree-Algorithm-(Algoritmo-Árvore-de-Decisão))\n",
    "    * [Entropy (Entropia)](#Entropy-(Entropia))\n",
    "    * [Information Gain (Ganho de Informação)](#Information-Gain-(Ganho-de-Informação))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09df0b6b-5f68-4335-9d0c-4cba237e5f01",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b9caffa-7c6d-481d-8412-934cbc62070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3995ae-c68c-4b21-8b1a-7f651de109f9",
   "metadata": {},
   "source": [
    "## Decision Tree Algorithm (Algoritmo Árvore de Decisão)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e86551db-46b3-4dfb-8ee4-bb02f6781c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[1, 1, 1],\n",
    "[0, 0, 1],\n",
    " [0, 1, 0],\n",
    " [1, 0, 1],\n",
    " [1, 1, 1],\n",
    " [1, 1, 0],\n",
    " [0, 0, 0],\n",
    " [1, 1, 0],\n",
    " [0, 1, 0],\n",
    " [0, 1, 0]])\n",
    "\n",
    "y_train = np.array([1, 1, 0, 0, 1, 1, 0, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0ddfe5-0770-4ee2-af04-d626d17c3aa1",
   "metadata": {},
   "source": [
    "### Entropy (Entropia)\n",
    "$$H(p_1) = -p_1 \\text{log}_2(p_1) - (1- p_1) \\text{log}_2(1- p_1)$$\n",
    "**[EN-US]**\n",
    "\n",
    "It is the measurement of the impurity of a dataset, it starts at 0, goes to 1 and returns to 0 depending on the fraction of positive examples in our sample.\n",
    "* $p_1$ = fraction of positive examples.\n",
    "* $p_0 = 1 - p_1$ = fraction of negative examples.\n",
    "\n",
    "We measure purity using a function called Entropy ($H(p_1)$). When our set is divided into 50%, that is, 50% label 1 and 50% label 0, we have $p_1 = 0.5$ and $H(p_1) = 1$, the worst possible value. When our set is divided into 100% with label 1 or 100% with label 0, we have $H(p_1) = 0$, an impurity of 0.\n",
    "\n",
    "Calculating entropy, we take the logs to base 2, rather than to $e$. We use $\\log_2$ just to make the peak of the curve equal to 1, if we used $\\log_e$, the base of natural lagarithms, this just scales this function vertically, it will still work, but the numbers become a little difficult to interpret, because the peak of the function is no longer a nice integer like 1.\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "É a medição da impureza (`Measuring Impurity`) de um dataset, começa em 0, vai até 1 e volta para 0 em função da fração de exemplos positivos em nossa amostra.\n",
    "* $p_1$ = fração dos exemplos positivos.\n",
    "* $p_0 = 1 - p_1$: Fração de exemplos negativos.\n",
    "\n",
    "Medimos a pureza usando uma função chamada Entropy ($H(p_1)$). Quando nosso set está dividido em 50%, ou seja, 50% label 1 e 50% label 0, temos $p_1 = 0.5$ e $H(p_1) = 1$, o pior valor possível. Quando nosso set está dividido em 100% com label 1 ou 100% com label 0, temos $H(p_1) = 0$, uma impureza de 0.\n",
    "\n",
    "Calculando a entropia, levamos os logs com base de 2, ao invés de para $e$. Usamos $\\log_2$ apenas para tornar o pico da curva igual a 1, caso usássemos $\\log_e$, a base dos lagaritmos naturais, isso apenas dimensiona verticalmente essa função, ainda funcionará, mas os números se tornam um pouco difíceis de interpretar, porque o pico da função não é mais um bom número inteiro como 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ee56a0e-c23b-4b8a-8c67-9d53b1ceccff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    entropy = .0\n",
    "\n",
    "    if len(y) != 0 or type(y) != numpy.float64:\n",
    "        p = len(y[y == 1]) / len(y)\n",
    "    \n",
    "        if p == 0 or p == 1:\n",
    "            entropy = .0\n",
    "        else:\n",
    "            entropy = -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n",
    "\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8d4c6e63-2bb4-41d3-8657-53b3e10dd2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(np.array([0.5, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0299a9e8-1a4e-4f0e-a0c2-377c34e89117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_indices(X, index_feature):\n",
    "    left_indices = []\n",
    "    right_indices = []\n",
    "    for i, x in enumerate(X):\n",
    "        if x[index_feature] == 1:\n",
    "            left_indices.append(i)\n",
    "        else:\n",
    "            right_indices.append(i)\n",
    "    return left_indices, right_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29f575a7-03b8-4219-9f00-f493c82cfbc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weighted_entropy(X, y, index_feature):\n",
    "    left_indices, right_indices = split_indices(X, index_feature)\n",
    "    \n",
    "    w_left = len(left_indices) / len(X)\n",
    "    w_right = len(right_indices) / len(X)\n",
    "    p_left = sum(y[left_indices]) / len(left_indices)\n",
    "    p_right = sum(y[right_indices]) / len(right_indices)\n",
    "\n",
    "    weighted_entropy = w_left * entropy(p_left) + w_right * entropy(p_right)\n",
    "    return weighted_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b91020d-1257-4647-9f5f-5088939f1c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_indices, right_indices = split_indices(X_train, 0)\n",
    "weighted_entropy(X_train, y_train, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c5900c-4eed-4fb2-b9f8-01259b13c8ae",
   "metadata": {},
   "source": [
    "### Information Gain (Ganho de Informação)\n",
    "$$\\text{Information Gain} = H(p_1^\\text{node})- (w^{\\text{left}}H(p_1^\\text{left}) + w^{\\text{right}}H(p_1^\\text{right}))$$\n",
    "**[EN-US]**\n",
    "\n",
    "The way we decide which feature to split into a node will be based on choosing the feature that reduces entropy the most. The reduction in entropy is called Information Gain.\n",
    "\n",
    "With this definition of entropy we can calculate the information gain associated with choosing any specific feature to divide at the node and choose the one with the highest information gain, increasing the purity of the data subsets in the left and right sub-branches.\n",
    "\n",
    "**[PT-BR]**\n",
    "\n",
    "A forma como decidiremos em qual feature dividir em um nó será baseada na escolha da feature que mais reduz a entropia. A redução da entropia é chamada de ganho de informação (Information Gain).\n",
    "\n",
    "Com essa definição de entropia podemos calcular o information gain associado à escolha de qualquer feature específica para dividir no nó e, escolher a que tem maior information gain, aumentando a pureza dos subsets dos dados nos sub-galhos da esquerda e direia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1c27c74-d04b-42ae-9d32-be6ca36f04e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def information_gain(X, y, index_feature):\n",
    "    p_node = sum(y) / len(y)\n",
    "    h_node = entropy(p_node)\n",
    "    w_entropy = weighted_entropy(X, y, index_feature)\n",
    "    return h_node - w_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d450ada-673e-43e9-b0c3-10e6203dfc7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2780719051126377"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_gain(X_train, y_train, left_indices, right_indices, entropy, weighted_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f807c36-e985-4741-868f-8bd4cb3ee3fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def best_split(X, y):\n",
    "    _, features = X.shape[1]\n",
    "    best_feature = -1\n",
    "\n",
    "    max_info_gain = 0\n",
    "    for feature in range(features):\n",
    "        gain = information_gain(X, y, feature)\n",
    "\n",
    "        if gain > max_infor_gain:\n",
    "            max_info_gain = gain\n",
    "            best_feature = feature\n",
    "\n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2733abdb-d90f-476e-8d61-54ad0ce281cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tree_recursive(X, y, branch_name, max_depth, current_depth):\n",
    "    if current_depth == max_depth:\n",
    "        formatting = ' ' * current_depth + '-' * current_depth\n",
    "        print(f'{formatting} {branch_name} leaf node with indices {list(range(X.shape[0]))}')\n",
    "        return\n",
    "\n",
    "    best_feature = best_split(X, y)\n",
    "    formatting = '-' * current_depth\n",
    "    print(f'{formatting} Depth {current_depth}, {branch_name}: Split on feature: {best_feature}')\n",
    "\n",
    "    left_indices, right_indices = split_indices(X, best_feature)\n",
    "    tree.append((left_indices, right_indices, best_feature))\n",
    "\n",
    "    tree_recursive(X[left_indices], y, 'Left', max_depth, current_depth+1)\n",
    "    tree_recursive(X[right_indices], y, 'Right', max_depth, current_depth+1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
